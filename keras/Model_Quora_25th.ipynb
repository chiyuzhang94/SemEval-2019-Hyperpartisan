{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "1. The original code is from https://github.com/aerdem4/kaggle-quora-dup\n",
    "\n",
    "2. The code was for kaggle competition, [Quora](https://www.kaggle.com/c/quora-question-pairs/overview)\n",
    "\n",
    "3. Here I haven't use **NLP feature** and **non NLP feature** from the code\n",
    "\n",
    "### To DO List\n",
    "1. Check Balance of True and False\n",
    "2. Punctuation remove\n",
    "3. simplize words\n",
    "> For exmpale\n",
    ">* was, is ,are -> be\n",
    ">* ketty -> cat\n",
    "\n",
    "4. Feature Engineering Generating\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Note** `#@@@@@@@@@@@@@` means the block is for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.merge import concatenate, add, multiply\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_pro =True\n",
    "np.random.seed(0)\n",
    "WNL = WordNetLemmatizer()\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "#Define the max lenght of the question\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "MIN_WORD_OCCURRENCE = 100\n",
    "REPLACE_WORD = \"memento\"\n",
    "EMBEDDING_DIM = 300\n",
    "NUM_FOLDS = 2\n",
    "BATCH_SIZE = 1025\n",
    "EMBEDDING_FILE = \"glove.840B.300d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Supported Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutter(word):\n",
    "    if len(word) < 4:\n",
    "        return word\n",
    "    return WNL.lemmatize(WNL.lemmatize(word, \"n\"), \"v\")\n",
    "\n",
    "\n",
    "def preprocess(string):\n",
    "    #*****************************************************************************\n",
    "    #how they know what catractore has to be replace? \n",
    "    string = string.lower().replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\") \\\n",
    "        .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\") \\\n",
    "        .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\") \\\n",
    "        .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\") \\\n",
    "        .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\") \\\n",
    "        .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \") \\\n",
    "        .replace(\"€\", \" euro \").replace(\"'ll\", \" will\").replace(\"=\", \" equal \").replace(\"+\", \" plus \")\n",
    "    string = re.sub('[“”\\(\\'…\\)\\!\\^\\\"\\.;:,\\-\\?？\\{\\}\\[\\]\\\\/\\*@]', ' ', string)\n",
    "    string = re.sub(r\"([0-9]+)000000\", r\"\\1m\", string)\n",
    "    string = re.sub(r\"([0-9]+)000\", r\"\\1k\", string)\n",
    "    string = ' '.join([cutter(w) for w in string.split()])\n",
    "    return string\n",
    "\n",
    "\n",
    "def get_embedding():\n",
    "    embeddings_index = {}\n",
    "    f = open(EMBEDDING_FILE)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if len(values) == EMBEDDING_DIM + 1 and word in top_words:\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def is_numeric(s):\n",
    "    return any(i.isdigit() for i in s)\n",
    "\n",
    "\n",
    "def prepare(q):\n",
    "    #return new_q, et(surplus_q), set(numbers_q)\n",
    "    # new_q: the new question inlcude top word and non-top word replaced by 'memento'\n",
    "    # set(surplus_q): a word bag that is not in STOP_WORDS and not in Top word\n",
    "    # set(numbers_q): a word bag of numbers. \n",
    "    q=str(q)\n",
    "    q=q.strip()\n",
    "\n",
    "    new_q = []\n",
    "    surplus_q = []\n",
    "    numbers_q = []\n",
    "    new_memento = True\n",
    "    \n",
    "    \n",
    "    # spilt each wrod in a article\n",
    "    #[::-1] means oppoite sort, last element becomes first one\n",
    "    for w in q.split()[::-1]:\n",
    "        if w in top_words:\n",
    "            new_q = [w] + new_q\n",
    "            new_memento = True\n",
    "        elif w not in STOP_WORDS:\n",
    "            if new_memento:\n",
    "                new_q = [\"memento\"] + new_q\n",
    "                new_memento = False\n",
    "            if is_numeric(w):\n",
    "                numbers_q = [w] + numbers_q\n",
    "            else:\n",
    "                surplus_q = [w] + surplus_q\n",
    "        else:\n",
    "            new_memento = True\n",
    "        if len(new_q) == MAX_SEQUENCE_LENGTH:\n",
    "            break\n",
    "    new_q = \" \".join(new_q)\n",
    "    \n",
    "\n",
    "    return new_q, set(surplus_q), set(numbers_q)\n",
    "\n",
    "\n",
    "#punchation problem\n",
    "def extract_features(df):\n",
    "    articles = np.array([\"\"] * len(df), dtype=object)\n",
    "    titles = np.array([\"\"] * len(df), dtype=object)\n",
    "    features = np.zeros((len(df), 4))\n",
    "\n",
    "    for i, (article, title) in enumerate(list(zip(df[\"article\"], df[\"title\"]))):\n",
    "        articles[i], surplus1, numbers1 = prepare(article)\n",
    "        titles[i], surplus2, numbers2 = prepare(title)\n",
    "        \n",
    "        features[i,0] = len(surplus1)\n",
    "        features[i,1]=len(numbers1)\n",
    "        features[i,2]=len(surplus2)\n",
    "        features[i,3]=len(numbers2)\n",
    "#         features[i, 0] = len(surplus1.intersection(surplus2))\n",
    "#         features[i, 1] = len(surplus1.union(surplus2))\n",
    "#         features[i, 2] = len(numbers1.intersection(numbers2))\n",
    "#         features[i, 3] = len(numbers1.union(numbers2))\n",
    "\n",
    "    return articles, titles, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    After DeVos Announced Plans To Reexamine Title...\n",
       "1    University To Award Trayvon Martin With Posthu...\n",
       "2    Texas State University suspends Greek life aft...\n",
       "3    Red Sox waste Rodriguez outing in 1-0 loss to ...\n",
       "4                            Eve and the New Jerusalem\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@@@@@@@@@@@@@@@@@\n",
    "train.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data prepare\n",
    "---\n",
    "\n",
    "\n",
    "### 3.1 Train, Test data \n",
    "* Reading data\n",
    "* Replace the words and clean the NA cells\n",
    "* Find the unique questions (Clean the duplicate data)\n",
    "* Convert all_questions to vector \n",
    "* Find Top Words, meaning find the words appear more than 100 times in all data(train+test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\")[0:200]\n",
    "test = pd.read_csv(\"../data/test.csv\")[0:200]\n",
    "# if simple_pro ==True:\n",
    "#     train=train[0:200].copy()\n",
    "#     test=test[0:200].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Find top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_article=pd.Series(train['article'].tolist()+test['article'].tolist())\n",
    "vectorizer = CountVectorizer(lowercase=False, token_pattern=\"\\S+\", min_df=MIN_WORD_OCCURRENCE,\n",
    "                             stop_words=STOP_WORDS)\n",
    "vectorizer.fit(all_article.tolist())\n",
    "\n",
    "#`top_words` will be use in function `get_embedding`\n",
    "top_words = set(vectorizer.vocabulary_.keys())\n",
    "top_words.add(REPLACE_WORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`max_df` is used for removing terms that appear too frequently, also known as \"corpus-specific stop words\". For example:\n",
    "\n",
    "> * max_df = 0.50 means \"ignore terms that appear in more than 50% of the documents\".\n",
    "* max_df = 25 means \"ignore terms that appear in more than 25 documents\".\n",
    "* The default max_df is 1.0, which means \"ignore terms that appear in more than 100% of the documents\". Thus, the default setting does not ignore any terms.\n",
    "\n",
    "`min_df` is used for removing terms that appear too infrequently. For example:\n",
    "\n",
    ">* min_df = 0.01 means \"ignore terms that appear in less than 1% of the documents\".\n",
    "* min_df = 5 means \"ignore terms that appear in less than 5 documents\".\n",
    "* The default min_df is 1, which means \"ignore terms that appear in less than 1 document\". * Thus, the default setting does not ignore any terms.\n",
    "\n",
    "\n",
    "[reference](https://stackoverflow.com/questions/27697766/understanding-min-df-and-max-df-in-scikit-countvectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@\n",
    "#1. 找轉換工具 for example:  is, are -> be\n",
    "#2. 清除沒意義的字\n",
    "#3. lower case ever words\n",
    "# print( len(top_words), len(vectorizer.vocabulary_.keys()))\n",
    "\n",
    "#show the contain of vectorizer\n",
    "for dic in list(vectorizer.vocabulary_.items())[:100]:\n",
    "    print(dic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Pre-Train vector: glove.840B.300d\n",
    "* Find the words from `top_words` in **glove.840B.300d** and extract words as vector\n",
    "> Retriving the vector from pre-trained model\n",
    "> 1. get the `top_words` \n",
    "> 2. make sure the `top_words` is in the pre-train model\n",
    "> 3. load the words which are in top_word and pre-train model **ONLY**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words are not found in the embedding: set()\n"
     ]
    }
   ],
   "source": [
    "#get the embedding word and coefficient from pre-train model.\n",
    "embeddings_index = get_embedding()\n",
    "\n",
    "# we can see some words that are not in the pre-train model,glove.840B.300d. \n",
    "print(\"Words are not found in the embedding:\", top_words - embeddings_index.keys())\n",
    "\n",
    "#reset top_words \n",
    "top_words = embeddings_index.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "---\n",
    "### Build up features\n",
    "The purpose of this section is to \n",
    "* Shape the question 1 and question 2 to the training format, so that we can feed them into the NN model. \n",
    "* Wrap the features together, **nlp_features**, **non_nlp featues**, **train_q_features**\n",
    "\n",
    "**The training data is the follwing:**\n",
    "> - article: \n",
    "- train_nlp_features: nlp features of traning data\n",
    "- train_non_nlp_features: non-nlp features of tranining data\n",
    "- train_q_features: \n",
    "\n",
    "\n",
    "### Function Explaination\n",
    "* `S.pad_sequences([[1,2,3]], maxlen=10, padding='post')`\n",
    " \n",
    "    return  `[[1, 2, 3, 0, 0, 0, 0, 0, 0, 0]]`\n",
    "\n",
    "\n",
    "* `tokenizer.texts_to_sequences` like str.split()\n",
    "\n",
    "    text= 'soem thing to eat'\n",
    "    \n",
    "    T = tokenizer.fit_on_texts(text)\n",
    "    \n",
    "    print( T.text_to_word_sequence(text1))  # ['some', 'thing', 'to', 'eat']\n",
    "\n",
    "    print( tokenizer.word_index)   #{'some': 1, 'thing': 2,'to': 3 ','eat': 4, drink': 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract_features\n",
    "\n",
    "* feature, 把suplus 轉成vector 去預測target\n",
    "* mix title and article as a new feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "articles_train, titles_train, features_train = extract_features(train)\n",
    "#建立tokenizer\n",
    "# extract word from all articles in Train\n",
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(articles_train)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# creat a mextrix: (num of article, 500)\n",
    "data_articles_train = pad_sequences(tokenizer.texts_to_sequences(articles_train), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_titles_train = pad_sequences(tokenizer.texts_to_sequences(titles_train), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels_train = np.array(train['hyperpartisan'])\n",
    "\n",
    "# features_train = np.hstack((features_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "articles_test, title_test, features_test =extract_features(test)\n",
    "data_articles_test = pad_sequences(tokenizer.texts_to_sequences(articles_test), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_titles_test = pad_sequences(tokenizer.texts_to_sequences(title_test), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels_test = np.array(test['hyperpartisan'])\n",
    "# features_test = np.hstack((features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the vector of words in pre-train model \n",
    "# that words has used in training set to embedding_matrix \n",
    "nb_words = len(word_index) + 1  # 3867\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model\n",
    "\n",
    "<img src=\"image/model.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def LSTM_model(): \n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "    lstm_layer = LSTM(75, recurrent_dropout=0.2)\n",
    "\n",
    "    # artile and title input formating\n",
    "    sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "    x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "    sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "    y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "    # feature input formating\n",
    "    features_input = Input(shape=(f_train.shape[1],), dtype=\"float32\")\n",
    "    features_dense = BatchNormalization()(features_input)\n",
    "    features_dense = Dense(200, activation=\"relu\")(features_dense)\n",
    "    features_dense = Dropout(0.2)(features_dense)\n",
    "\n",
    "    #blend article and title input by Square Differece\n",
    "    addition = add([x1, y1])\n",
    "    minus_y1 = Lambda(lambda x: -x)(y1)\n",
    "    merged = add([x1, minus_y1])\n",
    "    merged = multiply([merged, merged])\n",
    "    merged = concatenate([merged, addition])\n",
    "    merged = Dropout(0.4)(merged)\n",
    "\n",
    "    \n",
    "    merged = concatenate([merged, features_dense])\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = GaussianNoise(0.1)(merged)\n",
    "\n",
    "    merged = Dense(150, activation=\"relu\")(merged)\n",
    "    merged = Dropout(0.2)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "\n",
    "    out = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    model = Model(inputs=[sequence_1_input, sequence_2_input, features_input], outputs=out)\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=\"nadam\")\n",
    "    model.summary\n",
    "\n",
    "    return (model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: 0\n",
      "Train on 99 samples, validate on 101 samples\n",
      "Epoch 1/1\n",
      "99/99 [==============================] - 11s 109ms/step - loss: 0.8812 - val_loss: 1.0252\n",
      "0 validation loss: 1.0251847505569458\n",
      "200/200 [==============================] - 3s 15ms/step\n",
      "MODEL: 1\n",
      "Train on 101 samples, validate on 99 samples\n",
      "Epoch 1/1\n",
      "101/101 [==============================] - 11s 106ms/step - loss: 0.9100 - val_loss: 0.8869\n",
      "1 validation loss: 0.8869453072547913\n",
      "200/200 [==============================] - 3s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True)\n",
    "model_count = 0\n",
    "\n",
    "for idx_train, idx_val in skf.split(train, train['hyperpartisan']):\n",
    "    print(\"MODEL:\", model_count)\n",
    "    data_articles_intrain = data_articles_train[idx_train]\n",
    "    data_title_intrain = data_titles_train[idx_train]\n",
    "    labels_intrain = labels_train[idx_train]\n",
    "    f_intrain = features_train[idx_train]\n",
    "\n",
    "    data_articles_val = data_articles_train[idx_val]\n",
    "    data_title_val = data_titles_train[idx_val]\n",
    "    labels_val = labels_train[idx_val]\n",
    "    f_val = features_train[idx_val]\n",
    "    \n",
    "    model = LSTM_model()\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "    best_model_path = \"best_model\" + str(model_count) + \".h5\"\n",
    "    model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n",
    "    \n",
    "    \n",
    "    hist = model.fit([data_articles_intrain, data_title_intrain, f_intrain], labels_intrain,\n",
    "                     validation_data=([data_articles_val, data_title_val, f_val], labels_val),\n",
    "                     epochs=1, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                     callbacks=[early_stopping, model_checkpoint], verbose=1)\n",
    "\n",
    "    model.load_weights(best_model_path)\n",
    "    print(model_count, \"validation loss:\", min(hist.history[\"val_loss\"]))\n",
    "\n",
    "    preds = model.predict([data_articles_test, data_titles_test, features_test], batch_size=BATCH_SIZE, verbose=1)\n",
    "\n",
    "    model_count += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
