{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revised data_prepare for attention visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# the following codes are packaged as a py file that can be import in other python scripts. \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "# function for text cleaning\n",
    "def clean_text(text):\n",
    "    ## Remove puncuation\n",
    "    ## Convert words to lower case and split them\n",
    "    # replace non-readable apostrophes\n",
    "    # replace contractions of sequences as its original form .\n",
    "    text = text.lower().replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n",
    "   .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
    "   .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
    "   .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
    "   .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n",
    "   .replace(\"'ll\", \" will\").replace(\"how's\", \"how is\").replace(\"where's\", \"where is\")\n",
    "    #text = text.translate(string.punctuation)\n",
    "    token = word_tokenize(text)\n",
    "    text = text.translate(string.punctuation)\n",
    "    tokens2 = word_tokenize(text)\n",
    "    words = [word for word in tokens2 if word.isalpha()] # remove non-alphabatical words\n",
    "\n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    word_filter = [w for w in words if not w in stops and len(w) >= 3]\n",
    "    text_return = \" \".join(word_filter)\n",
    "    return text_return,token\n",
    "# function for data loading\n",
    "def load_data(dataset,data_size):\n",
    "    articles = []\n",
    "    token_all = []\n",
    "    i = 0\n",
    "    with open(dataset) as f:\n",
    "        lines = f.readlines()\n",
    "        for item in lines[-data_size:]:\n",
    "            #print(item)\n",
    "            seq,token= clean_text(item)\n",
    "            articles.append(seq)\n",
    "            token_all.append(token)\n",
    "            i += 1\n",
    "            if i%100 == 0:\n",
    "                print(str(i))\n",
    "    return articles,token_all\n",
    "# function for labels loading\n",
    "def load_labels(labelset,data_size):\n",
    "    labels = []\n",
    "    with open(labelset) as f:\n",
    "        lines = f.readlines()\n",
    "        for item in lines[-data_size:]:\n",
    "            labels.append(int(item.split(\"\\n\")[0]))\n",
    "    labels = np.asarray(labels)\n",
    "    return labels\n",
    "# function for tokenization and padding\n",
    "def tokenize(articles,vocabulary_size,sequence_length,load_model,tokenizer):\n",
    "    if load_model == False:\n",
    "        tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "        tokenizer.fit_on_texts(articles) # create a dictionary that has words in articles as key and the index as value\n",
    "    sequences = tokenizer.texts_to_sequences(articles)\n",
    "    data = pad_sequences(sequences, maxlen = sequence_length,padding='pre', truncating='post') # pad the article that has less words than the sequence_length to sequence_length using zeros\n",
    "    return data,tokenizer\n",
    "# function for data spliting\n",
    "def split_data(data,label_array,train_size):\n",
    "    X_train = data[0:train_size,:]\n",
    "    y_train = label_array[0:train_size]\n",
    "\n",
    "    X_test = data[train_size:,:]\n",
    "    y_test = label_array[train_size:]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "# call data prepare functions and return results\n",
    "def data_ready(dataset, labelset,data_size,vocabulary_size,sequence_length,train_size,load_model,tokenizer):\n",
    "    articles,token = load_data(dataset,data_size)\n",
    "    label_array = load_labels(labelset,data_size)\n",
    "    data,tokenizer = tokenize(articles,vocabulary_size,sequence_length,load_model,tokenizer)\n",
    "    X_train, y_train, X_test, y_test = split_data(data, label_array, train_size)\n",
    "    return X_train, y_train, X_test, y_test, tokenizer\n",
    "\n",
    "# function for pre-trained word embedding loading\n",
    "# tokenizer is needed here to map the word in google w2v file with the words in the articles.\n",
    "def load_w2v(w2v_file, binary, vocabulary_size, embedding_dim,tokenizer):\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(w2v_file, binary=binary)\n",
    "    w2v = word_vectors\n",
    "\n",
    "    embeddings_index = dict()\n",
    "    vocab = w2v.vocab.keys()\n",
    "    for word in vocab:\n",
    "        coefs = np.asarray(w2v.word_vec(word), dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "    embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index > vocabulary_size - 1:\n",
    "            continue\n",
    "        else:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[index] = embedding_vector\n",
    "    return embedding_matrix\n",
    "# function for word transform\n",
    "# to get the vector of all words in articles of batch_size\n",
    "def trans2input_batch_first(batch_data, batch_size, sequence_length,embedding_dim, embedding_matrix):\n",
    "    zero_vc = np.zeros(embedding_dim,dtype='float32')\n",
    "    input_data = np.zeros((batch_size, sequence_length,embedding_dim))\n",
    "    for i in range(batch_size):\n",
    "        # print(i)\n",
    "        for j in range(sequence_length):\n",
    "            indx = batch_data[i,j]\n",
    "            if indx != 0:\n",
    "                input_data[i,j,:] = embedding_matrix[indx]\n",
    "            else:\n",
    "                input_data[i,j,:] = zero_vc\n",
    "    return input_data\n",
    "# to get the vector of all words in articles of batch_size\n",
    "def trans2input_batch_second(batch_data, batch_size, sequence_length,embedding_dim, embedding_matrix):\n",
    "    zero_vc = np.zeros(embedding_dim,dtype='float32')\n",
    "    input_data = np.zeros((sequence_length,batch_size,embedding_dim))\n",
    "    for i in range(batch_size):\n",
    "        # print(i)\n",
    "        for j in range(sequence_length):\n",
    "            indx = batch_data[i,j]\n",
    "            if indx != 0:\n",
    "                input_data[j,i,:] = embedding_matrix[indx]\n",
    "            else:\n",
    "                input_data[j,i,:] = zero_vc\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following codes are packaged as a py file that can be import in other python scripts. \n",
    "# this user own package is named as \"test\"\n",
    "import torch\n",
    "# define a function for model test with pre-trained word2vec\n",
    "def test_with_w2v(test_set,test_labels,data_size,vocabulary_size,sequence_length,load_model,tokenizer,batch_size,embedding_dim,embedding_matrix,model,batch_first):\n",
    "    # load data and labels\n",
    "    articles,token = load_data(test_set,data_size)\n",
    "    labels = load_labels(test_labels,data_size)\n",
    "    # tokenize and transform to matrix\n",
    "    X, tokenizer = tokenize(articles,vocabulary_size,sequence_length,load_model,tokenizer)\n",
    "    all_weights = []\n",
    "    all_true_label = []\n",
    "    pred_labels = [] \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    acc = 0\n",
    "    # predict label for test data by given model\n",
    "    for i in range(0,data_size,batch_size):# mini batch process\n",
    "        if batch_first: \n",
    "            input_x = trans2input_batch_first(X[i:i+batch_size,:],batch_size,sequence_length,embedding_dim,embedding_matrix) # word embedding using google news vectors\n",
    "        else:\n",
    "            input_x = trans2input_batch_second(X[i:i+batch_size,:],batch_size,sequence_length,embedding_dim,embedding_matrix) # word embedding using google news vectors\n",
    "\n",
    "        b_x = torch.from_numpy(input_x).float()   # reshape x to (batch, time_step, input_size)\n",
    "    \n",
    "        test_output,attn_weights = model(b_x)  # model output\n",
    "        all_weights.extend(attn_weights)\n",
    "        \n",
    "        y_true = labels[i:i+batch_size]\n",
    "        all_true_label.extend(y_true)\n",
    "        \n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze() # to get the maximum probability of very article in the batch and get its value\n",
    "        pred_labels.extend(pred_y.tolist())\n",
    "        \n",
    "        total += y_true.shape[0] # the total number of article in the test data\n",
    "        correct += (pred_y == y_true).sum().item() # the number of intances that pred_y matches with the y_true\n",
    "        \n",
    "    acc = 100.00 * float(correct) / float(total) # get accuracy    \n",
    "    return acc,output_dic,X,token,all_weights,all_true_label,pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:68: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 86.66666666666667\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from time import time\n",
    "import pickle\n",
    "import json\n",
    "torch.manual_seed(1)    # reproducible\n",
    "\n",
    "# get data    \n",
    "test_set = \"test_article.txt\"\n",
    "test_label = \"test_label.txt\"\n",
    "load_model = True\n",
    "load_model_path = \"./pytorch_lstmattn_2\"\n",
    "w2v_file = \"GoogleNews-vectors-negative300.bin\" \n",
    "w2v_bi = True\n",
    "\n",
    "# use GPU \n",
    "cuda_gpu = False\n",
    "\n",
    "batch_first = True\n",
    "data_size = 300 # the number of articles in dataset\n",
    "# Hyper Parameters\n",
    "batch_size = 5  # Batch size\n",
    "vocabulary_size = 40000 # The number of unique words is \n",
    "sequence_length = 436 # The number of words per article \n",
    "embedding_dim = 300  # Dimension of word embedding \n",
    "hidden_dim = 500 # The number of unit in a hidden layer\n",
    "num_layers = 1 # The number of hidden layers \n",
    "dropout = 0.0  # The dropout rate\n",
    "output_size = 2 # The output size \n",
    "lr = 0.001           # learning rate\n",
    "# don't change, test always needs to load model\n",
    "load_model = True\n",
    "# built a lstm structure\n",
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim,num_layers,output_size,dropout):\n",
    "        super(AttentionLSTM,self).__init__() # don't forget to call this!\n",
    "        \n",
    "        self.encoder = nn.LSTM(input_size=embedding_dim, # assign size of each input data\n",
    "                             hidden_size=hidden_dim,\n",
    "                             num_layers = num_layers,\n",
    "                           batch_first = True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1=nn.Linear(hidden_dim,output_size)\n",
    "        self.hidden2out = nn.Linear(hidden_dim,output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "    \n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        \"\"\" \n",
    "        Now we will incorporate Attention mechanism in our LSTM model. In this new model, we will use attention to compute soft alignment score corresponding\n",
    "        between each of the hidden_state and the last hidden_state of the LSTM. We will be using torch.bmm for the batch matrix multiplication.\n",
    "        \"\"\"\n",
    "        hidden = final_state.permute(1,2,0)\n",
    "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2)\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        return new_hidden_state,soft_attn_weights\n",
    "  \n",
    "    def forward(self, x):\n",
    "        output, (final_hidden_state, final_cell_state) = self.encoder(x,None)\n",
    "       \n",
    "        attn_output,soft_attn_weights = self.attention_net(output, final_hidden_state)\n",
    "       \n",
    "        attn_output= self.dropout(attn_output)\n",
    "        fc_output= self.fc1(attn_output)\n",
    "        output = self.softmax(fc_output) # output layer using softmax function\n",
    "        return output,soft_attn_weights\n",
    "\n",
    "if cuda_gpu:\n",
    "    lstmattn = AttentionLSTM(embedding_dim, hidden_dim,num_layers,output_size,dropout).cuda()\n",
    "else:\n",
    "    lstmattn = AttentionLSTM(embedding_dim, hidden_dim,num_layers,output_size,dropout)\n",
    "\n",
    "lstmattn.load_state_dict(torch.load(load_model_path, map_location='cpu'))\n",
    "with open('./output/lstmattn_tokenizer.pickle', 'rb') as handle:\n",
    "\ttokenizer = pickle.load(handle)\n",
    "# using google news w2v as word embedding model\n",
    "embedding_matrix = load_w2v(w2v_file, w2v_bi, vocabulary_size, embedding_dim,tokenizer) # use the google w2v vector as the embedding layer\n",
    "# use test function in test package, get test accurcy, precision, recall and f1 score\n",
    "acc,output_dic,X,token,all_weights,all_true_label,pred_labels = test_with_w2v(test_set,test_label,data_size,vocabulary_size,sequence_length,load_model,tokenizer,batch_size,embedding_dim,embedding_matrix,lstmattn,batch_first)\n",
    "print(\"accuracy\",acc)\n",
    "# store results\n",
    "with open('./output/test_output_lstmattn.json', 'w') as outfile2:\n",
    "    json.dump(output_dic, outfile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['details', 'national', 'missing', 'unidentified', 'persons', 'system', 'hard', 'use', 'meanwhile', 'local', 'officials', 'often', 'strapped', 'funds', 'run', 'autopsies', 'dna', 'tests', 'faces', 'bureaucratic', 'hurdles', 'getting', 'dna', 'samples', 'family', 'members', 'back', 'home', 'countries', 'missing', 'missing', 'person', 'reports', 'filed', 'foreign', 'consulates', 'often', 'make', 'system', 'date', 'lab', 'identified', 'three', 'four', 'brooks', 'county', 'location', 'series', 'mass', 'graves', 'told', 'remains', 'people', 'recovered', 'talking', 'people', 'voice', 'alive', 'even', 'less', 'dead', 'baker', 'said', 'reel', 'took', 'idea', 'publisher', 'could', 'observer', 'fundraise', 'build', 'searchable', 'tool', 'would', 'allow', 'families', 'view', 'items', 'found', 'remains', 'make', 'connection', 'missing', 'loved', 'ones', 'rosary', 'among', 'belongings', 'held', 'baylor', 'university', 'laboratory', 'forensic', 'anthropologists', 'seeking', 'identify', 'migrants', 'remains', 'photo', 'jen', 'reel', 'courtesy', 'texas', 'observer', 'bosses', 'agreed', 'campaign', 'underway', 'project', 'tengo', 'nombre', 'journalism', 'crowdfunding', 'platform', 'beacon', 'observer', 'aims', 'raise', 'general', 'public', 'matching', 'funds', 'beacon', 'pledged', 'spend', 'million', 'projects', 'site', 'money', 'observer', 'raises', 'would', 'towards', 'paying', 'app', 'developer', 'logging', 'assistant', 'spanish', 'translator', 'latter', 'two', 'roles', 'may', 'filled', 'one', 'person', 'wednesday', 'morning', 'project', 'way', 'goal', 'nine', 'days', 'left', 'deadline', 'fundraising', 'target', 'reached', 'next', 'goal', 'app', 'live', 'summer', 'observer', 'working', 'consular', 'officials', 'human', 'rights', 'organizations', 'groups', 'develop', 'instructions', 'app', 'users', 'think', 'made', 'match', 'separately', 'baker', 'building', 'relationships', 'officials', 'mexico', 'honduras', 'guatemala', 'salvador', 'try', 'overcome', 'bureaucratic', 'hurdles', 'collecting', 'dna', 'samples', 'families', 'strikingly', 'observer', 'project', 'actually', 'second', 'recent', 'instance', 'journalists', 'using', 'talents', 'try', 'address', 'problem', 'unidentified', 'dead', 'center', 'investigative', 'reporting', 'reveal', 'news', 'september', 'launched', 'tool', 'lost', 'amp', 'found', 'also', 'designed', 'version', 'observer', 'effort', 'focused', 'migrants', 'reveal', 'project', 'seeks', 'match', 'unidentified', 'bodies', 'missing', 'persons', 'reveal', 'data', 'team', 'led', 'jennifer', 'michael', 'corey', 'scraped', 'data', 'federal', 'database', 'used', 'build', 'website', 'users', 'browse', 'search', 'entries', 'missing', 'dead', 'side', 'side', 'try', 'make', 'match', 'find', 'potential', 'match', 'users', 'encouraged', 'click', 'button', 'submits', 'news', 'given', 'advice', 'whether', 'contact', 'authorities', 'reveal', 'stresses', 'contacting', 'authorities', 'users', 'staff', 'journalist', 'schulz', 'whose', 'reporting', 'series', 'accompanies', 'tool', 'describes', 'project', 'version', 'local', 'stations', 'public', 'safety', 'hotline', 'turns', 'one', 'people', 'end', 'particular', 'matthews', 'director', 'case', 'management', 'communications', 'convinced', 'tool', 'value', 'though', 'matthews', 'former', 'factory', 'worker', 'started', 'amateur', 'says', 'little', 'sense', 'encouraging', 'members', 'public', 'try', 'make', 'matches', 'breakthroughs', 'come', 'law', 'enforcement', 'come', 'families', 'missing', 'far', 'none', 'roughly', 'potential', 'matches', 'received', 'useful', 'matthews', 'said', 'optimistic', 'observer', 'plans', 'reveal', 'editors', 'acknowledge', 'tool', 'limitations', 'data', 'public', 'users', 'cir', 'tool', 'could', 'find', 'potential', 'matches', 'would', 'rule', 'says', 'senior', 'editor', 'fernando', 'diaz', 'couple', 'dozen', 'wrong', 'matches', 'matthews', 'received', 'nature', 'dream', 'able', 'help', 'somebody', 'make', 'match', 'maybe', 'spend', 'years', 'researching', 'diaz', 'says', 'organization', 'focused', 'impact', 'set', 'goal', 'early', 'help', 'make', 'one', 'match', 'worth', 'observer', 'reel', 'colleagues', 'describe', 'task', 'way', 'journalism', 'per', 'says', 'observer', 'editor', 'forrest', 'wilder', 'could', 'think', 'service', 'ultimately', 'reason', 'hope', 'stories', 'come', 'app', 'fact', 'hundreds', 'people', 'dying', 'backyards', 'texas', 'closure', 'loved', 'ones', 'human', 'tragedy', 'wilder', 'said', 'help', 'way', 'skills', 'expertise', 'want']\n"
     ]
    }
   ],
   "source": [
    "seq = np.ndarray.tolist(X[1:])[0]\n",
    "word = []\n",
    "for item in seq:\n",
    "    if item == 0:\n",
    "        word.append(\"{PAD}\")\n",
    "    else:\n",
    "        word.append(index_word[item])\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'nixonian', 'end', 'to', 'the', '2016', 'sleaze', 'sweepstakes', '?', 'opinion', 'modal', 'trigger', 'hillary', 'clinton', 'as', 'the', 'presidential', 'campaigns', 'sink', 'to', 'the', 'challenge', 'of', 'demonstrating', 'that', 'there', 'is', 'no', 'such', 'thing', 'as', 'rock', 'bottom', ',', 'remember', 'this', ':', 'when', 'the', 'clintons', 'decamped', 'from', 'washington', 'in', 'january', '2001', ',', 'they', 'took', 'some', 'white', 'house', 'furnishings', 'that', 'were', 'public', 'property', '.', 'they', 'also', 'finished', 'accepting', 'more', 'than', '$', '190,000', 'in', 'gifts', ',', 'including', 'two', 'coffee', 'tables', 'and', 'two', 'chairs', ',', 'a', '$', '7,375', 'gratuity', 'from', 'denise', 'rich', ',', 'whose', 'fugitive', 'former', 'husband', 'had', 'been', 'pardoned', 'in', 'president', 'clinton', 'own', 'final', 'hours', '.', 'a', 'washington', 'post', 'editorial', '(', '“', 'count', 'the', 'spoons', '”', ')', 'identified', '“', 'the', 'clintons', \"'\", 'defining', 'characteristic', ':', 'they', 'have', 'no', 'capacity', 'for', 'embarrassment', '.', 'words', 'like', 'shabby', 'and', 'tawdry', 'come', 'to', 'mind', '.', 'they', 'do', 'not', 'begin', 'to', 'do', 'it', 'justice.', '”', 'today', ',', 'hillary', 'clinton', 'strives', 'to', 'live', 'again', 'among', 'some', 'white', 'house', 'furnishings', 'that', 'she', 'and', 'her', 'helpmeet', 'were', 'compelled', 'to', 'disgorge', ',', 'her', 'campaign', 'flounders', 'because', 'as', 'secretary', 'of', 'state', 'some', 'of', 'the', 'nation', 'own', 'business', 'might', 'have', 'been', 'melded', 'with', 'the', 'contents', 'of', 'a', 'computer', 'that', 'is', 'pertinent', 'to', 'an', 'fbi', 'investigation', 'of', 'a', 'former', 'democratic', 'congressman', 'own', 'alleged', 'sexual', 'texting', 'with', 'a', 'female', 'minor', '.', 'ransack', 'the', 'english', 'language', 'for', 'words', 'to', 'do', 'this', 'justice', '.', 'during', 'the', 'recent', 'welter', 'of', 'reports', 'about', 'the', 'clintons', \"'\", 'self-dealing', 'through', 'their', 'charity', 'that', 'has', 'been', 'very', 'charitable', 'to', 'them', ',', 'the', 'new', 'yorker', ',', 'reporting', 'her', 'plans', 'to', 'uplift', 'the', 'downtrodden', ',', 'quoted', 'her', 'aspiration', ':', '“', 'i', 'want', 'to', 'really', 'marry', 'the', 'public', 'and', 'the', 'private', 'sector.', '”', 'this', 'would', 'solve', 'the', 'clintons', \"'\", 'problem', 'of', 'discerning', 'the', 'line', 'between', 'public', 'business', 'and', 'private', 'aggrandizement', ':', 'erase', 'the', 'line', '.', 'so', ',', 'herewith', 'america', 'own', 'choice', '.', 'restore', 'the', 'house', 'of', 'clinton', '.', 'or', 'confer', 'executive', 'powers', '—', 'powers', 'that', 'president', 'obama', 'by', 'his', 'audacity', ',', 'and', 'congress', 'by', 'its', 'lethargy', ',', 'have', 'proven', 'to', 'be', 'essentially', 'unlimited', '—', 'on', 'another', 'competitor', 'in', 'the', 'sleaze', 'sweepstakes', ',', 'donald', 'trump', ',', 'who', 'shares', 'his', 'opponent', 'own', 'disinclination', 'to', 'disentangle', 'the', 'personal', 'and', 'the', 'political', '.', 'into', 'this', 'political', 'maelstrom', ',', 'fbi', 'director', 'james', 'comey', 'injected', 'an', 'announcement', 'that', 'intensified', 'the', 'chaos', 'without', 'providing', 'a', 'scintilla', 'of', 'news', 'that', 'voters', 'can', 'use', ':', 'an', 'unknown', 'number', 'of', 'emails', 'of', 'unknown', 'provenance', 'and', 'unknown', 'content', 'might', 'be', '“', 'pertinent', '”', 'to', 'an', 'investigation', 'that', 'already', 'has', 'established', ',', 'beyond', 'peradventure', ',', 'that', 'secretary', 'clinton', 'was', '“', 'extremely', 'careless', '”', 'with', 'sensitive', 'material', '.', 'add', 'the', 'fbi', ',', 'and', 'the', 'justice', 'department', 'to', 'which', 'it', 'belongs', ',', 'to', 'the', 'carnage', 'of', 'institutions', 'that', 'is', 'a', 'byproduct', 'of', 'bad', 'judgments', 'by', 'the', 'political', 'class', 'that', 'have', 'voters', 'asking', 'casey', 'stengel', 'own', 'question', '.', 'in', '1962', ',', 'stengel', ',', 'manager', 'of', 'the', 'new', 'york', 'mets', ',', 'an', 'expansion', 'team', 'en', 'route', 'to', 'a', 'record', 'of', '120', 'losses', ',', 'looked', 'down', 'his', 'dugout', 'at', 'his', 'woebegone', 'players', 'and', 'wondered', 'aloud', ',', '“', 'can', 'not', 'anybody', 'here', 'play', 'this', 'game', '”', 'it', 'is', 'grimly', 'hilarious', 'to', 'hear', 'it', 'said', 'that', 'the', 'justice', 'department', ',', 'by', 'not', 'holding', 'comey', 'to', 'established', 'protocols', 'concerning', 'discussions', 'of', 'ongoing', 'investigations', ',', 'and', 'concerning', 'pronouncements', 'close', 'to', 'elections', ',', 'has', 'tainted', 'itself', '.', 'obamacare', 'would', 'not', 'have', 'passed', 'if', 'justice', 'department', 'lawyers', 'had', 'not', 'conducted', 'what', 'a', 'federal', 'judge', 'declared', 'a', 'corrupt', 'prosecution', 'of', 'alaska', 'own', 'republican', 'senator', 'ted', 'stevens', ',', 'costing', 'him', 're-election', '.', 'the', 'department', 'has', 'enabled', ',', 'by', 'not', 'seriously', 'investigating', ',', 'the', 'irs', \"'\", 'suppression', 'of', 'political', 'advocacy', 'by', 'conservative', 'groups', '.', 'or', 'of', 'the', 'irs', \"'\", 'subsequent', 'destruction', 'of', 'subpoenaed', 'emails', 'pertinent', 'to', 'this', '.', 'so', ',', 'unsurprisingly', ',', 'the', 'most', 'intrusive', 'and', 'potentially', 'punitive', 'federal', 'agency', 'continues', 'to', 'punish', 'conservative', 'groups', 'for', 'being', 'conservative', ',', 'according', 'to', 'cleta', 'mitchell', ',', 'a', 'lawyer', 'for', 'political', 'groups', 'who', 'confirms', 'there', 'are', 'indeed', 'conservative', 'organizations', 'that', 'were', 'targeted', 'by', 'the', 'irs', 'and', 'have', 'still', 'not', 'received', 'their', 'tax-exempt', 'status', '.', 'in', '2013', ',', 'barack', 'obama', 'professed', 'himself', '“', 'angry', '”', 'about', '“', 'inexcusable', '”', 'irs', 'behavior', ',', 'before', 'he', 'decided', 'there', 'was', 'not', 'a', '“', 'smidgen', '”', 'of', 'irs', 'corruption', '.', 'he', 'claimed', 'to', 'have', 'learned', 'about', 'the', 'irs', 'behavior', 'from', 'the', 'media', '.', 'now', 'he', 'claims', 'that', 'he', 'learned', 'from', 'the', 'media', 'about', 'clinton', 'own', 'email', 'abuses', ',', 'although', 'they', 'had', 'exchanged', 'emails', 'using', 'her', 'private', 'server', '.', 'perhaps', '.', 'the', 'defining', 'scandal', 'of', 'the', 'obama', 'era', 'has', 'been', 'the', 'media', 'own', 'lackadaisical', 'consensus', 'that', 'obama', 'own', 'administration', 'has', 'had', 'no', 'serious', 'scandal', '.', 'this', ',', 'although', 'with', 'the', 'justice', 'department', 'protecting', 'the', 'irs', ',', 'the', 'administration', 'has', '(', 'in', 'the', 'words', 'of', 'richard', 'nixon', 'own', 'white', 'house', 'counsel', 'john', 'dean', ')', 'used', '“', 'the', 'available', 'federal', 'machinery', 'to', 'screw', 'our', 'political', 'enemies.', '”', 'clinton', ',', 'the', 'ultimate', 'author', 'of', 'her', 'current', 'agony', ',', 'resembles', 'no', 'one', 'so', 'much', 'as', 'nixon', 'in', 'her', 'paranoia', 'and', 'joyless', 'pursuit', 'of', 'joy', '.', 'her', 'government', 'career', 'began', 'with', 'the', 'house', 'committee', 'preparing', 'nixon', 'own', 'impeachment', '.', 'twenty-two', 'years', 'earlier', ',', 'he', 'had', 'saved', 'his', 'career', 'by', 'addressing', 'a', 'supposed', 'scandal', 'with', 'his', 'nationally', 'broadcast', '“', 'checkers', 'speech', ',', '”', 'which', 'was', 'mawkish', ',', 'abasing', 'and', 'effective', '.', 'how', 'fitting', 'it', 'would', 'be', 'for', 'a', 'clinton', '“', 'checkers', 'speech', '”', 'to', 'end', 'our', 'long', 'national', 'nightmare', 'that', 'this', 'campaign', 'has', 'been', '.']\n"
     ]
    }
   ],
   "source": [
    "#example of tokens\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Open visualization.html to checkout the attention coefficients visualization.\n"
     ]
    }
   ],
   "source": [
    "# visulize attention weights on texts by html\n",
    "index_word = {v: k for k, v in tokenizer.word_index.items()}\n",
    "with open(\"visualization_manual.html\", \"w\") as html_file:\n",
    "    for i in range(len(X)):\n",
    "        match_ind = 0\n",
    "        html_file.write('<br><br><br>')\n",
    "        label = all_true_label[i]\n",
    "        pre = pred_labels[i]\n",
    "        seq = np.ndarray.tolist(X[i:])[0]\n",
    "        word = []\n",
    "        for item in seq:\n",
    "            if item == 0:\n",
    "                word.append(\"{PAD}\")\n",
    "            else:\n",
    "                word.append(index_word[item])\n",
    "        \n",
    "        weight = all_weights[i].data.numpy()\n",
    "        weight = weight / weight.max()\n",
    "        \n",
    "        if label == 1:\n",
    "            html_file.write('<p>Label is Hyperpartisan, Predict as %s</p>' % (pre))\n",
    "            for w in token[i]:\n",
    "                if w in word[match_ind:]:\n",
    "                    match_ind = word[match_ind:].index(w)+match_ind\n",
    "                    alpha = weight[match_ind]\n",
    "                    html_file.write('<font style=\"background: rgba(255, 0, 0, %f)\">%s</font>\\n' % (alpha, w))\n",
    "                else:\n",
    "                    html_file.write('<font style=\"background: rgba(255, 0, 0, %f)\">%s</font>\\n' % (0.0, w))\n",
    "                \n",
    "        elif label == 0:\n",
    "            html_file.write('<p>Label is Non-hyperpartisan, Predict as %s</p>' % (pre))\n",
    "            for w in token[i]:\n",
    "                if w in word[match_ind:]:\n",
    "                    match_ind = word[match_ind:].index(w)+match_ind\n",
    "                    alpha = weight[match_ind]\n",
    "                    html_file.write('<font style=\"background: rgba(0, 0, 255, %f)\">%s</font>\\n' % (alpha, w))\n",
    "                else:\n",
    "                    html_file.write('<font style=\"background: rgba(0, 0, 255, %f)\">%s</font>\\n' % (0.0, w))\n",
    "print('\\nOpen visualization.html to checkout the attention coefficients visualization.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
